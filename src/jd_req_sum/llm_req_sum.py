import requests
import pandas as pd
import json
import time
import os
import glob

# --- C·∫•u h√¨nh ---
API_KEYS = [

]

DELAY_BETWEEN_REQUESTS_SECONDS = 2

SYSTEM_PROMPT = """
You are an expert in text analysis. Your task is to read a list of skill-related sentences belonging to the same cluster and summarize them into a single representative sentence, following these requirements:

- Produce an abstractive summary; do not copy any sentence verbatim.
- Select only one core skill that best represents the entire cluster.
- If there are synonymous expressions, keep only the most common phrasing.
- For software tools, always normalize the format to "using [tool]".
- Do not use parentheses, do not add explanations, and do not provide comments.
- Output must be a single concise line.

List of skills:
{skill_list}

Output format: one single concise summary line only.
"""



# ------------------------------------------------------------------------------------------
# 1. FUNCTION: G·ªçi Qwen-14B qua OpenRouter
# ------------------------------------------------------------------------------------------
def call_qwen_summary(api_key, skills_list, key_index=None):
    """
    G·ªçi Qwen-14B th√¥ng qua OpenRouter API ƒë·ªÉ t√≥m t·∫Øt danh s√°ch k·ªπ nƒÉng.
    """
    try:
        formatted_skills = "\n".join([f'"{s}",' for s in skills_list])
        final_prompt = SYSTEM_PROMPT.format(skill_list=formatted_skills)

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://your-app.com",
            "X-Title": "Skill Summarization",
        }

        payload = {
            "model": "qwen/qwen-14b-chat",
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": final_prompt}
            ],
            "temperature": 0
        }

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            json=payload,
            headers=headers,
            timeout=60
        )

        if response.status_code == 200:
            content = response.json()["choices"][0]["message"]["content"]
            return content.strip()

        # L·ªói quota/throttle t·ª´ OpenRouter
        if response.status_code == 429:
            return f"ERROR: 429 Rate limit for API key #{key_index+1}"

        # L·ªói key h·∫øt h·∫°n
        if response.status_code in [401, 403]:
            return f"ERROR: API key #{key_index+1} expired or unauthorized."

        return f"ERROR: HTTP {response.status_code}: {response.text}"

    except Exception as e:
        return f"ERROR: Exception calling Qwen: {str(e)}"


# ------------------------------------------------------------------------------------------
# 2. Retry logic ƒë∆°n gi·∫£n cho OpenRouter
# ------------------------------------------------------------------------------------------
def get_skill_summary_with_retry(api_key, skills_list, key_index=None, max_retries=3):
    for i in range(max_retries):
        result = call_qwen_summary(api_key, skills_list, key_index)

        if "429" not in result:
            return result

        wait_time = (i + 1) * 15
        print(f"‚ö†Ô∏è API key #{key_index+1} b·ªã rate limit. Ch·ªù {wait_time} gi√¢y...")
        time.sleep(wait_time)

    return result


# ------------------------------------------------------------------------------------------
# 3. X·ª¨ L√ù CLUSTERING ‚Äî ƒê√É B·ªé SKIP FILE
# ------------------------------------------------------------------------------------------
def process_clustering_results():

    results_folder = ""
    output_folder = ""

    os.makedirs(output_folder, exist_ok=True)

    # L·∫•y t·∫•t c·∫£ file
    result_files = glob.glob(os.path.join(results_folder, "clustering_results_*.json"))

    if not result_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file clustering_results_*.json")
        return

    print(f"üìÑ T√¨m th·∫•y {len(result_files)} file, s·∫Ω x·ª≠ l√Ω to√†n b·ªô (kh√¥ng skip):")
    for f in result_files:
        print(" -", os.path.basename(f))

    key_count = len(API_KEYS)
    exhausted_keys = set()

    for file_idx, result_file in enumerate(result_files):

        base_name = os.path.basename(result_file)
        output_name = base_name.replace("clustering_results_", "skill_summary_")
        output_path = os.path.join(output_folder, output_name)

        print("\n" + "=" * 80)
        print(f"üîç ƒêANG X·ª¨ L√ù FILE: {base_name}")
        print("=" * 80)

        with open(result_file, "r", encoding="utf-8") as f:
            clustering_data = json.load(f)

        clusters = clustering_data.get("clusters", [])

        print(f"‚Üí T√¨m th·∫•y {len(clusters)} clusters")

        # load progress
        existing = {}
        processed_ids = set()

        if os.path.exists(output_path):
            try:
                with open(output_path, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                for item in existing.get("skill_summaries", []):
                    processed_ids.add(item["cluster_id"])
                print(f"‚Üí File output ƒë√£ c√≥, {len(processed_ids)} cluster ƒë√£ x·ª≠ l√Ω.")
            except:
                existing = {}

        if not existing:
            existing = {
                "metadata": {
                    "source_file": base_name,
                    "timestamp": clustering_data.get("metadata", {}).get("timestamp", ""),
                    "total_clusters": len(clusters),
                    "processing_timestamp": pd.Timestamp.now().isoformat()
                },
                "skill_summaries": []
            }

        remaining_clusters = [
            c for c in clusters if c["cluster_id"] not in processed_ids
        ]

        print(f"‚Üí C√≤n {len(remaining_clusters)} clusters ch∆∞a x·ª≠ l√Ω.")

        for cluster_idx, cluster in enumerate(remaining_clusters):

            cluster_id = cluster["cluster_id"]
            sentences = cluster["sentences"]

            print(f"\nüîπ Cluster {cluster_id} ({len(sentences)} sentences)")

            if len(exhausted_keys) == key_count:
                print("‚ùå T·∫•t c·∫£ API keys ƒë√£ exhausted ‚Üí D·ª´ng.")
                break

            # ch·ªçn key
            key_index = (cluster_idx + file_idx * 100) % key_count
            while key_index in exhausted_keys:
                key_index = (key_index + 1) % key_count

            api_key = API_KEYS[key_index]

            # g·ªçi LLM
            summary = get_skill_summary_with_retry(api_key, sentences, key_index)

            # key l·ªói
            if "expired" in summary or "unauthorized" in summary:
                exhausted_keys.add(key_index)
                print(f"‚ùå API key #{key_index+1} h·∫øt h·∫°n ‚Üí skip key.")
                continue

            if "429" in summary:
                exhausted_keys.add(key_index)
                print(f"‚ùå API key #{key_index+1} rate limit ‚Üí skip key.")
                continue

            # l∆∞u k·∫øt qu·∫£
            existing["skill_summaries"].append({
                "cluster_id": cluster_id,
                "original_sentences_count": len(sentences),
                "original_sentences": sentences,
                "skill_summary": summary,
                "processing_key": f"API_KEY_{key_index+1}"
            })

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)

            print(f"‚úÖ Summary: {summary}")

            time.sleep(DELAY_BETWEEN_REQUESTS_SECONDS)

        print(f"üéâ Ho√†n th√†nh file: {base_name}")


def main():
    process_clustering_results()


if __name__ == "__main__":
    main()
